{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Улучшение выборки данных для обучения.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNMTsVu2JwVna4pilazJpDv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlinaZakharova1997/miramedix_practice/blob/main/%D0%A3%D0%BB%D1%83%D1%87%D1%88%D0%B5%D0%BD%D0%B8%D0%B5_%D0%B2%D1%8B%D0%B1%D0%BE%D1%80%D0%BA%D0%B8_%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85_%D0%B4%D0%BB%D1%8F_%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D1%8F.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Изначально было собрано 10000 текстов, из них 3303 оказались дубликатами. Поэтому пришлось пересобрать файлы train, val, test.\n",
        "Выгрузили их с диска, собрали в один список all_data, это получился список вложенных словарей, которые преобразовали в просто список словарей new_data, чтобы получилось сделать из него датафрейм df. Из датафрейма с помощью функции drop_duplicates() были автоматически удалены все дубликаты. В итоге получилось 6697 текстов. Их поделили заново на три части train, val, test в том же соотношении 0,5:0,25:0,25 и сформировали новые файлы jsonl для обучения и проверки модели."
      ],
      "metadata": {
        "id": "Gg37MWDe4r05"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7A_1xJqm3nLV"
      },
      "outputs": [],
      "source": [
        "!pip install jsonlines"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jsonlines\n",
        "import json\n",
        "import pandas as pd "
      ],
      "metadata": {
        "id": "CZZvU_St30VP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "hUE-_6JY36Jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_data = []\n",
        "\n",
        "with jsonlines.open('/content/drive/MyDrive/test_data.json', 'r') as reader:\n",
        "  for obj in reader:\n",
        "    if obj not in all_data:\n",
        "      all_data.append(obj)\n",
        "  \n",
        "with jsonlines.open('/content/drive/MyDrive/val_data.json', 'r') as reader:\n",
        "  for obj in reader:\n",
        "    if obj not in all_data:\n",
        "      all_data.append(obj)\n",
        "\n",
        "with jsonlines.open('/content/drive/MyDrive/train_data.json', 'r') as reader:\n",
        "  for obj in reader:\n",
        "    if obj not in all_data:\n",
        "      all_data.append(obj)"
      ],
      "metadata": {
        "id": "K8AWe6Nv3-bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_data = []\n",
        "for item in all_data:\n",
        "  pairs = {}\n",
        "  pairs['de'] = item['translation']['de']\n",
        "  pairs['en'] = item['translation']['en']\n",
        "  new_data.append(pairs)"
      ],
      "metadata": {
        "id": "dnuKyv-14JyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(new_data)"
      ],
      "metadata": {
        "id": "D01BEmjK4RUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop_duplicates(keep='first')"
      ],
      "metadata": {
        "id": "PXoSdkA44Xt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "for i in range(len(df)):\n",
        "  translation = {}\n",
        "  langs = {}\n",
        "  langs['de'] = df['de'][i]\n",
        "  langs['en'] = df['en'][i]\n",
        "  translation['translation'] = langs\n",
        "  #print(langs)\n",
        "  data.append(translation)"
      ],
      "metadata": {
        "id": "cyrGNyHH4YWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = data[:3347]\n",
        "val = data[3347:5020]\n",
        "test = data[5020:]"
      ],
      "metadata": {
        "id": "mragS1T74mA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with jsonlines.open('test_data.jsonl', mode='w') as writer:\n",
        "    writer.write_all(test)\n",
        "with jsonlines.open('val_data.jsonl', mode='w') as writer:\n",
        "    writer.write_all(val)\n",
        "with jsonlines.open('train_data.jsonl', mode='w') as writer:\n",
        "    writer.write_all(train)"
      ],
      "metadata": {
        "id": "oqdedhHJ4oxe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}